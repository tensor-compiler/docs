<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="favicon.ico" />
    <title>Computing on Tensors - TACO Documentation</title>
    <link rel="stylesheet" href="css/theme.css" />
    <link rel="stylesheet" href="css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
        <link href="extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Computing on Tensors";
        var mkdocs_page_input_path = "pycomputations.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="." class="icon icon-home"> TACO Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">C++ API</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="tensors.html">Defining Tensors</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="computations.html">Computing on Tensors</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="scheduling.html">Providing a Schedule</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Python API</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="pytensors.html">Defining Tensors</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="pycomputations.html">Computing on Tensors</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#specifying-tensor-algebra-computations">Specifying Tensor Algebra Computations</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#expressing-reductions">Expressing Reductions</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#expressing-broadcasts">Expressing Broadcasts</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#expressing-transposes">Expressing Transposes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#performing-the-computation">Performing the Computation</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="" href="http://tensor-compiler.org/reference/">Reference Manual</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="tutorial.html">Tutorial</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Example Applications</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="scientific_computing.html">Scientific Computing: SpMV</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="data_analytics.html">Data Analytics: MTTKRP</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="machine_learning.html">Machine Learning: SDDMM</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Advanced Usage</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="optimization.html">Strategies for Optimization</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Guide to Benchmarking</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="controlling_memory.html">Controlling Memory</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href=".">TACO Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="." class="icon icon-home" alt="Docs"></a> &raquo;</li>
          <li>Python API &raquo;</li>
      <li>Computing on Tensors</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="computing-on-tensors">Computing on Tensors</h1>
<h2 id="specifying-tensor-algebra-computations">Specifying Tensor Algebra Computations</h2>
<p>Tensor algebra computations can be expressed in TACO using tensor index
notation, which at a high level describes how each element in the result tensor
can be computed from elements in the operand tensors. As an example, matrix
addition can be expressed in index notation as </p>
<p>
<script type="math/tex; mode=display">A_{ij} = B_{ij} + C_{ij}</script>
</p>
<p>where <script type="math/tex">A</script>, <script type="math/tex">B</script>, and <script type="math/tex">C</script> denote two-dimensional tensors (i.e., matrices)
while <script type="math/tex">i</script> and <script type="math/tex">j</script> are index variables that represent abstract indices into
the corresponding dimensions of the tensors.  In plain English, the example
above essentially states that, for every <script type="math/tex">i</script> and <script type="math/tex">j</script>, the element in the
<script type="math/tex">i</script>-th row and <script type="math/tex">j</script>-th column of <script type="math/tex">A</script> should be assigned the sum of the
corresponding elements in <script type="math/tex">B</script> and <script type="math/tex">C</script>. Similarly, element-wise
multiplication of three tensors can be expressed in index notation as </p>
<p>
<script type="math/tex; mode=display">A_{ijk} = B_{ijk} \cdot C_{ijk} \cdot D_{ijk}.</script>
</p>
<p>To define the same computation using the TACO Python API, we can write very
similar code, with the main difference being that we also have to explicitly
declare the index variables beforehand:</p>
<pre class="highlight"><code class="language-python">i, j, k = pytaco.index_var(), pytaco.index_var(), pytaco.index_var()
A[i,j,k] = B[i,j,k] * C[i,j,k] * D[i,j,k]</code></pre>
<p>This can also be rewritten more compactly as</p>
<pre class="highlight"><code class="language-python">i, j, k = pytaco.get_index_vars(3)
A[i,j,k] = B[i,j,k] * C[i,j,k] * D[i,j,k]</code></pre>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Accesses to scalars also require the square brackets notation.  Since
scalars are equivalent to tensors with zero dimension, <code>None</code> must be
explicitly specified as indices to indicate that no index variable is
needed to access a scalar.  As an example, the following expresses the
addition of two scalars <code>beta</code> and <code>gamma</code>:</p>
<pre class="highlight"><code class="language-python">alpha[None] = beta[None] + gamma[None]</code></pre>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>TACO currently does not support computations that have a tensor as both an 
operand and the result, such as the following:</p>
<pre class="highlight"><code class="language-python">a[i] = a[i] * b[i]</code></pre>
<p>Such computations can be rewritten using explicit temporaries as follows:</p>
<pre class="highlight"><code class="language-python">t[i] = a[i] * b[i]
a[i] = t[i]</code></pre>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>TACO currently does not support using the same index variable to index into 
multiple dimensions of the same tensor operand (e.g., <code>A[i,i]</code>).</p>
</div>
<h3 id="expressing-reductions">Expressing Reductions</h3>
<p>In all of the previous examples, all the index variables are used to index into
both the result and the operands of a computation.  It is also possible for
an index variable to be used to index into the operands only, in which case the
dimension indexed by that index variable is reduced (summed) over. For 
instance, the computation </p>
<p>
<script type="math/tex; mode=display">y_{i} = A_{ij} \cdot x_{j}</script>
</p>
<p>can be rewritten with the summation more explicit as </p>
<p>
<script type="math/tex; mode=display">y_{i} = \sum_{j} A_{ij} \cdot x_j</script>
</p>
<p>and demonstrates how matrix-vector multiplication can be expressed in index
notation.  Both forms are supported by TACO:</p>
<pre class="highlight"><code class="language-python">i, j = pytaco.get_index_vars(2)

y[i] = A[i,j] * x[j]
y[i] = pytaco.sum(j, A[i,j] * x[j])</code></pre>
<p>Reductions that are not explicitly expressed are assumed to be over the
smallest subexpression that captures all uses of the corresponding reduction
variable. For instance, the computation </p>
<p>
<script type="math/tex; mode=display">y_{i} = A_{ij} \cdot x_{j} + z_{i}</script>
</p>
<p>is equivalent to </p>
<p>
<script type="math/tex; mode=display">y_i = \big(\sum_{j} A_{ij} \cdot x_j\big) + z_i,</script>
</p>
<p>whereas the computation </p>
<p>
<script type="math/tex; mode=display">y_{i} = A_{ij} \cdot x_{j} + z_{j}</script>
</p>
<p>is equivalent to </p>
<p>
<script type="math/tex; mode=display">y_i = \sum_{j} \big(A_{ij} \cdot x_j + z_j\big).</script>
</p>
<h3 id="expressing-broadcasts">Expressing Broadcasts</h3>
<p>TACO supports computations that broadcasts tensors along any number of
dimensions.  The following example, for instance, broadcasts the vector <code>c</code> 
along the row dimension of matrix <code>B</code>, adding <code>c</code> to each row of <code>B</code>:</p>
<pre class="highlight"><code class="language-python">A[i, j] =  B[i, j] + c[j]</code></pre>
<p>However, TACO does not support NumPy-style broadcasting of dimensions that have 
a size of one.  For example, the following is not allowed:</p>
<pre class="highlight"><code class="language-python">A = pt.tensor([3,3])
B = pt.tensor([3,3])
C = pt.tensor([3,1])
i, j = pt.get_index_vars(2)

A[i, j] =  B[i, j] + C[i, j]  # ERROR!!</code></pre>
<h3 id="expressing-transposes">Expressing Transposes</h3>
<p>Computations that transpose tensors can be expressed by rearranging the order 
in which index variables are used to access tensor operands.  The following
example, for instance, adds matrix <code>B</code> to the transpose of matrix <code>C</code> and
stores the result in matrix <code>A</code>:</p>
<pre class="highlight"><code class="language-python">A = pt.tensor([3,3], pt.format([dense, dense]))
B = pt.tensor([3,3], pt.format([dense, dense]))
C = pt.tensor([3,3], pt.format([dense, dense]))
i, j = pt.get_index_vars(2)

A[i,j] = B[i,j] + C[j,i]</code></pre>
<p>Note, however, that sparse dimensions of tensor operands impose dependencies on
the order in which they can be accessed, based on the order in which they are
stored in the operand formats.  This means, for instance, that if <code>B</code> is a CSR
matrix, then <code>B[i,j]</code> requires that the dimension indexed by <code>i</code> be accessed
before the dimension indexed by <code>j</code>.  TACO does not support any computation
where these constraints form a cyclic dependency.  So the same computation from
before is not supported for CSR matrices, since the access of <code>B</code> requires that
<code>i</code> be accessed before <code>j</code> but the access of <code>C</code> requires that <code>j</code> be accessed
before <code>i</code>:</p>
<pre class="highlight"><code class="language-python">A = pt.tensor([3,3], pt.format([dense, compressed]))
B = pt.tensor([3,3], pt.format([dense, compressed]))
C = pt.tensor([3,3], pt.format([dense, compressed]))
i, j = pt.get_index_vars(2)

A[i,j] = B[i,j] + C[j,i]  # ERROR!!</code></pre>
<p>As an alternative, you can first explicitly transpose <code>C</code> by invoking its
<code>transpose</code> method, storing the result in a temporary, and then perform the
addition with the already-transposed temporary:</p>
<pre class="highlight"><code class="language-python">A = pt.tensor([3,3], pt.format([dense, compressed]))
B = pt.tensor([3,3], pt.format([dense, compressed]))
C = pt.tensor([3,3], pt.format([dense, compressed]))
i, j = pt.get_index_vars(2)

Ct = C.transpose([1, 0])  # Ct is also stored in the CSR format
A[i,j] = B[i,j] + Ct[i,j]</code></pre>
<p>Similarly, the following computation is not supported for the same reason that
the access of <code>B</code>, which is stored in row-major order, requires <code>i</code> to be
accessed before <code>j</code> but the access of <code>C</code>, which is stored in column-major
order, requires <code>j</code> to be accessed before <code>i</code>:</p>
<pre class="highlight"><code class="language-python">A = pt.tensor([3,3], pt.format([dense, compressed]))
B = pt.tensor([3,3], pt.format([dense, compressed]))
C = pt.tensor([3,3], pt.format([dense, compressed], [1, 0]))
i, j = pt.get_index_vars(2)

A[i,j] = B[i,j] + C[i,j]  # ERROR!!</code></pre>
<p>We can again perform the same computation by invoking <code>transpose</code>, this time to
repack <code>C</code> into the same CSR format as <code>A</code> and <code>B</code> before computing the 
addition:</p>
<pre class="highlight"><code class="language-python">A = pt.tensor([3,3], pt.format([dense, compressed]))
B = pt.tensor([3,3], pt.format([dense, compressed]))
C = pt.tensor([3,3], pt.format([dense, compressed], [1, 0]))
i, j = pt.get_index_vars(2)

Cp = C.transpose([0, 1], pt.format([dense, compressed]))  # Store a copy of C in the CSR format
A[i,j] = B[i,j] + Cp[i,j]</code></pre>
<h2 id="performing-the-computation">Performing the Computation</h2>
<p>Once a tensor algebra computation has been defined, you can simply invoke the
result tensor's <code>evaluate</code> method to perform the actual computation:</p>
<pre class="highlight"><code class="language-python">A.evaluate()</code></pre>
<p>Under the hood, TACO will first invoke the result tensor's <code>compile</code>
method to generate code that performs the computation.  TACO will then perform 
the actual computation by first invoking <code>assemble</code> to compute the sparsity 
structure of the result and subsequently invoking <code>compute</code> to compute the 
values of the result's nonzero elements.  Of course, you can also manually 
invoke these methods in order to more precisely control when each step happens:</p>
<pre class="highlight"><code class="language-python">A.compile()
A.assemble()
A.compute()</code></pre>
<p>If you define a computation and then access the result without first manually
invoking <code>evaluate</code> or <code>compile</code>/<code>assemble</code>/<code>compute</code>, TACO will automatically
invoke the computation immediately before the result is accessed.  In the
following example, for instance, TACO will automatically generate code to
compute the vector addition and then also actually perform the computation
right before <code>a[0]</code> is printed:</p>
<pre class="highlight"><code class="language-python">a[i] = b[i] + c[i]
print(a[0])</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="pytensors.html" class="btn btn-neutral float-left" title="Defining Tensors"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="tutorial.html" class="btn btn-neutral float-right" title="Tutorial">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="pytensors.html" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="tutorial.html" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '.';</script>
    <script src="js/theme_extra.js" defer></script>
    <script src="js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
