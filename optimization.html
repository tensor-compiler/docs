<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="favicon.ico" />
    <title>Strategies for Optimization - TACO Documentation</title>
    <link rel="stylesheet" href="css/theme.css" />
    <link rel="stylesheet" href="css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
        <link href="extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Strategies for Optimization";
        var mkdocs_page_input_path = "optimization.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="." class="icon icon-home"> TACO Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">C++ API</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="tensors.html">Defining Tensors</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="computations.html">Computing on Tensors</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="scheduling.html">Providing a Schedule</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Python API</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="pytensors.html">Defining Tensors</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="pycomputations.html">Computing on Tensors</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="http://tensor-compiler.org/reference/">Reference Manual</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="tutorial.html">Tutorial</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Example Applications</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="scientific_computing.html">Scientific Computing: SpMV</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="data_analytics.html">Data Analytics: MTTKRP</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="machine_learning.html">Machine Learning: SDDMM</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Advanced Usage</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="optimization.html">Strategies for Optimization</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#selecting-the-right-tensor-format">Selecting the Right Tensor Format</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#parallelizing-computations">Parallelizing Computations</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#fusing-computations">Fusing Computations</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Guide to Benchmarking</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="controlling_memory.html">Controlling Memory</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href=".">TACO Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="." class="icon icon-home" alt="Docs"></a> &raquo;</li>
          <li>Advanced Usage &raquo;</li>
      <li>Strategies for Optimization</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="strategies-for-optimization">Strategies for Optimization</h1>
<p>This section describes various strategies for improving the performace of
applications that use TACO to perform linear and tensor algebra computations.</p>
<h2 id="selecting-the-right-tensor-format">Selecting the Right Tensor Format</h2>
<p>TACO supports storing tensors in a wide range of formats, including many
commonly used ones like dense arrays, compressed sparse row (CSR), and
compressed sparse fiber (CSF).  Using the right formats to store a sparse
computation's operands and result can not only reduce the amount of memory
needed to perform the computation but also improve its performance.  In
particular, by selecting formats that accurately describe the sparsity and
structure of the operands, TACO can generate code under the hood that exploits
these properties of the data to avoid redundantly computing with zero elements
and thus speed up a computation.</p>
<p>As previously <a href="pytensors.html#defining-tensor-formats">explained</a>, TACO uses a
novel scheme that describes different tensor storage formats by specifying
whether each dimension is sparse or dense.  A dense dimension indicates to TACO
that most if not all slices of the tensor along that dimension contain at least
one nonzero element.  So if every element in a matrix is nonzero, we can make
that explicit by storing the matrix in a format where both dimensions are
dense, which indicates that every row is nonempty and that every column in each
row stores a nonzero element:</p>
<pre class="highlight"><code class="language-python">pytaco.format([pytaco.dense, pytaco.dense])  # a.k.a. a dense array</code></pre>
<p>A sparse dimension, on the other hand, indicates to TACO that most slices of
the tensor along that dimension contain only zeros.  So if relatively few rows
of a matrix is nonempty and if relatively few columns in each nonempty row
store nonzero elements, we can also make that explicit by storing the matrix in
a format where both dimensions are sparse:</p>
<pre class="highlight"><code class="language-python">pytaco.format([pytaco.compressed, pytaco.compressed])  # a.k.a. a DCSR matrix</code></pre>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Storing a tensor dimension as a sparse dimension incurs overhead that is 
proportional to the number of nonempty slices along that dimension, so only 
do so if most slices are actually empty.  Otherwise, it is more appropriate 
to store the dimension as a dense dimension.</p>
</div>
<p>It is easy to define custom formats for storing tensors with complex
sparsity structures.  For example, let's say we have a three-dimensional
tensor <script type="math/tex">A_{ijk}</script> that has no empty slice along the <script type="math/tex">K</script> dimension, and let's
say that each row in a slice is either entirely empty (i.e., <script type="math/tex">A_{ijk} = 0</script>
for all <script type="math/tex">j</script> and some fixed <script type="math/tex">k</script>, <script type="math/tex">i</script>) or entirely full (i.e., <script type="math/tex">A_{ijk}
\neq 0</script> for all <script type="math/tex">j</script> and some fixed <script type="math/tex">k</script>, <script type="math/tex">i</script>).  Following the same scheme
as before, we can define a tensor format that stores dimension 2 (i.e., the
<script type="math/tex">K</script> dimension) as a dense dimension, stores dimension 0 (i.e., the <script type="math/tex">I</script>
dimension) of each slice along dimension 2 as a sparse dimension, and stores
dimension 1 (i.e., the <script type="math/tex">J</script> dimension) of each nonempty row as a dense
dimension also:</p>
<pre class="highlight"><code class="language-python">pytaco.format([pytaco.dense, pytaco.compressed, pytaco.dense], [2, 0, 1])</code></pre>
<p>Using the format above, we can then efficiently store <script type="math/tex">A</script> without explicitly 
storing any zero element.</p>
<p>As a rough rule of thumb, using formats that accurately describe the sparsity
and structure of the operands lets TACO minimize memory traffic incurred to
load tensors from memory as well as minimize redundant work done to perform a
computation, which boosts performance.  This is particularly the case when only
one operand is sparse and the computation does not involve adding elements of
multiple operands.  <strong>This is not a hard and fast rule though.</strong>  In
particular, computing with multiple sparse operands might prevent TACO from 
applying some optimizations like <a href="#parallelizing-computations">parallelization</a> 
that might otherwise be possible if some of those operands were stored in dense 
formats.  Depending on how sparse your data actually is, this may or may not 
negatively impact performance.</p>
<p>The most reliable way to determine what are the best formats for storing
tensors in your application is to just try out many different formats and see
what works. Fortunately, as the examples above demonstrate, this is simple to
do with TACO.</p>
<h2 id="parallelizing-computations">Parallelizing Computations</h2>
<p>By default, TACO performs all computations using a single thread.  The maximum
number of threads that TACO may use to perform computations can be adjusted by
calling the <code>pytaco.set_num_threads</code> function.  The example below, for
instance, tells TACO that up to four threads may be used to execute any
subsequent computation in parallel if possible:</p>
<pre class="highlight"><code class="language-python">pytaco.set_num_threads(4)</code></pre>
<p>In general, the maximum number of threads for performing computations should
not be set greater than the number of available processor cores.  And depending
on the specific computation and characteristics of the data, setting the
maximum number of threads to be less than the number of processor cores may
actually yield better performance.  As the example above demonstrates, TACO 
makes it easy to try out different numbers of threads and see what works best 
for your application.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Setting the maximum number of available threads to be greater than one does
not guarantee that all computations will be executed in parallel.  In
particular, TACO will not execute a computation in parallel if </p>
<ul>
<li>it multiplies two or more sparse operands (e.g. sparse vector
  multiplication) or adds a sparse operand to any other operand (e.g.,
  adding two DCSR matrices), unless the outermost dimensions of the sparse
  operands are stored as dense dimensions (e.g., adding two CSR matrices,
  which can be parallelized if the result is stored in a dense matrix); </li>
<li>the first dimension it has to iterate over is one that is supposed to
  reduce over (e.g., multiplying a CSC matrix by a vector, which requires
  iterating over the column dimension of the matrix before the row
  dimension even though the column dimension is reduced over); or</li>
<li>it stores the result in a sparse tensor format.</li>
</ul>
<p>If TACO does not seem to be executing a computation in parallel, using 
different formats to store the operands and result may help.</p>
</div>
<p>By default, when performing computations in parallel, TACO will assign the same
number of coordinates along a particular dimension to be processed by each
thread.  For instance, when adding two 1000-by-1000 matrices using two threads,
TACO will have each thread compute exactly 500 rows of the result.  This would 
be inefficient though if, for instance, all the nonzeros are stored in the
first 500 rows of the operands, since one thread would end up doing all the
work while the other thread does nothing.  In cases like this, an alternative
parallelization strategy can be specified by calling the
<code>pytaco.set_parallel_schedule</code> function:</p>
<pre class="highlight"><code class="language-python">pt.set_parallel_schedule("dynamic") </code></pre>
<p>In contrast to the default parallelization strategy, the dynamic strategy will
have each thread first compute just one row of the result.  Whenever a thread
finishes computing a row, TACO will assign another row for that thread to
compute, and this process is repeated until all 1000 rows have been computed.
In this way, work is guaranteed to be evenly distributed between the two
threads regardless of the sparsity structures of the operands.</p>
<p>Using a dynamic strategy for parallel execution will incur some overhead
though, since work is assigned to threads at runtime.  This overhead can be
reduced by increasing the chunk size, which is the amount of additional work
that is assigned to a thread whenever it completes its previously assigned
work.  The example below, for instance, tells TACO to assign ten additional
rows of the result (instead of just one) for a thread to compute whenever it
has completed the previous ten rows:</p>
<pre class="highlight"><code class="language-python">pt.set_parallel_schedule("dynamic", 10) </code></pre>
<p>Since dynamic parallelization strategies incur additional overhead, whether or
not using them improves the performance of a computation will depend on how
evenly spread out the nonzero elements in the tensor operands are.  If each
matrix contains roughly the same number of nonzeros in every row, for instance,
then using a dynamic strategy will likely not more evenly distribute work
between threads.  In that case, the default static schedule would likely yield
better performance.</p>
<h2 id="fusing-computations">Fusing Computations</h2>
<p>TACO supports efficiently computing complicated tensor algebra expressions
involving many discrete operations in a single shot.  Let's say, for instance,
that we would like to (element-wise) add two vectors <code>b</code> and <code>c</code> and compute
the cosine of each element in the sum.  We can, of course, simply compute the
addition and the cosine of the sum in separate statements:</p>
<pre class="highlight"><code class="language-python">t[i] = b[i] + c[i]
a[i] = pt.cos(t[i])</code></pre>
<p>The program above will first invoke TACO to add <code>b</code> and <code>c</code>, store the result
into a temporary vector <code>t</code>, and then invoke TACO again to compute the cosine
of every element in <code>t</code>.  Performing the computation this way though not only
requires additional memory for storing <code>t</code> but also requires accessing the
memory subsystem to first write <code>t</code> to memory and then load <code>t</code> back from
memory, which is inefficient if the vectors are large and cannot be stored in
cache.  Instead, we can compute the addition and the cosine of the sum in a 
single statement:</p>
<pre class="highlight"><code class="language-python">a[i] = pt.cos(b[i] + c[i])</code></pre>
<p>For the program above, TACO will automatically generate code that, for every
<code>i</code>, immediately computes the cosine of <code>b[i] + c[i]</code> as soon as the sum is
computed.  TACO thus avoids storing the sum of <code>b</code> and <code>c</code> in a temporary
vector, thereby increasing the performance of the computation.</p>
<p>Fusing computations can improve performance if it does not require intermediate
results to be recomputed multiple times, as is the case with the previous
example.  Let's say, however, that we would like to multiply a matrix <code>B</code> by a
vector <code>c</code> and then multiply another matrix <code>A</code> by the result of the first
multiplication.  As before, we can express both operations in a single
statement:</p>
<pre class="highlight"><code class="language-python">y[i] = A[i,j] * B[j,k] * x[k]</code></pre>
<p>In this case though, computing both operations in one shot would require that
the multiplication of <code>B</code> and <code>x</code> be redundantly recomputed for every
(non-empty) row of <code>A</code>, thus reducing performance.  By contrast, computing the
two matrix-vector multiplications in separate statement ensures that the result
of the first matrix-vector multiplication does not have to be redundantly
computed, thereby minimizing the amount of work needed to perform the
computation:</p>
<pre class="highlight"><code class="language-python">t[j] = B[j,k] * c[k]
y[i] = A[i,j] * t[j]</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="machine_learning.html" class="btn btn-neutral float-left" title="Machine Learning: SDDMM"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="benchmarking.html" class="btn btn-neutral float-right" title="Guide to Benchmarking">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="machine_learning.html" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="benchmarking.html" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '.';</script>
    <script src="js/theme_extra.js" defer></script>
    <script src="js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
